{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/a_level_1/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2483bdb755b64c1da72c798983ec76f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from qdrant_client.models import ScalarQuantization, ScalarQuantizationConfig, ScalarType\n",
    "from qdrant_client.http.models import PointStruct, VectorParams, Distance\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from qdrant_client.http.models import SearchParams\n",
    "from qdrant_client import QdrantClient\n",
    "from faster_whisper import WhisperModel\n",
    "from pydub.utils import make_chunks\n",
    "from datasets import load_dataset\n",
    "from pydub import AudioSegment\n",
    "import moviepy.editor as mp\n",
    "from openai import OpenAI\n",
    "from copy import deepcopy\n",
    "import simpleaudio as sa\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import time\n",
    "import uuid\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "\n",
    "tedlium = load_dataset(\"LIUM/tedlium\", \"release1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"queries.json\", \"r\") as f:\n",
    "    queries = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Point to the local server\n",
    "client = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\")\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"lmstudio-community/Meta-Llama-3-8B-Instruct-GGUF\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You will be given one query and k sentences. For each sentence, you need to determine if it is a similar text to the query. If it is similar semantically, you should mark it as 1. If it is not similar, you should mark it as 0. You have to return a list of 0s and 1s (where 0 means different, and 1 means similar) of length k, like the following: [0, 1, 1, 0, 0, 1]. YOU MUST ONLY RETURN THE LIST OF THE RELEVANCE AS YOUR ANSWER. Note: similarity means that the query and the sentence talk about one things, or have one idea, or similar structurally, or similar semantically, or have one meaning.\"},\n",
    "    {}\n",
    "    {\"role\": \"user\", \"content\": \"Query: \" + queries[0][\"query\"] + \"\\nSentences:\\n\" + \"\\n\".join([\"0: \" + queries[0]['original'], \"1: \" + queries[1]['original']])},\n",
    "  ],\n",
    "  temperature=0.4,\n",
    "  max_tokens=100\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: why a specific entity like persephone's marriage contract, rather than another arbitrary one?\n",
      "Sentences:\n",
      "the explanatory role of  persephone  's marriage contract  could be played equally well by  infinitely many  other  ad hoc  entities  why a marriage contract  and not  any  other reason  for \n",
      "here are three questions  that i like to use  to test the truthiness  of our representation  in any  media story  one \n"
     ]
    }
   ],
   "source": [
    "print(\"Query: \" + queries[0][\"query\"] + \"\\nSentences:\\n\" + \"\\n\".join([queries[0]['original'], queries[1]['original']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "why a specific entity like persephone's marriage contract, rather than another arbitrary one?\n",
      "the explanatory role of  persephone  's marriage contract  could be played equally well by  infinitely many  other  ad hoc  entities  why a marriage contract  and not  any  other reason  for \n"
     ]
    }
   ],
   "source": [
    "print(queries[0][\"query\"])\n",
    "print(queries[0][\"original\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'audio': {'path': None, 'array': array([-0.0012207 , -0.00518799,  0.00765991, ...,  0.00442505,\n",
      "       -0.00942993, -0.01245117]), 'sampling_rate': 16000}, 'text': \"<sil> so {SMACK} for(2) example {BREATH} there are(2) doctors in china who believe that it's their job to keep you healthy {BREATH} so any month <sil> you are healthy you pay them(2) <sil> and when(4) you're sick {SMACK} you don't have(2) to pay them because(2) they failed at their job\", 'speaker_id': 'DerekSivers_2009I', 'gender': 2, 'file': '/Users/citadel/.cache/huggingface/datasets/downloads/extracted/261c9fe9ed6072b9d54e979afcd1b8bcc2a2f043991ddd463ce8ebe931129998/train/DerekSivers_2009I.sph', 'id': 'DerekSivers_2009I-105.78-114.78-<o,f0,male>'}\n"
     ]
    }
   ],
   "source": [
    "print(tedlium[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_braces_and_lower_from_batch(row):\n",
    "    # Pattern to match content inside parentheses, curly braces, and square braces including the braces\n",
    "    pattern = r'\\(.*?\\)|\\{.*?\\}|\\[.*?\\]|\\<.*?\\>'\n",
    "    transformed_row = deepcopy(row)\n",
    "    # Substitute the matches with an empty string\n",
    "    for i in range(len(row['text'])):\n",
    "        cleaned_text = re.sub(pattern, '', row['text'][i])\n",
    "        transformed_row['text'][i] = cleaned_text.lower()\n",
    "    \n",
    "    return transformed_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tedlium.set_transform(remove_braces_and_lower_from_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "embeddings = model.encode(tedlium[\"train\"][\"text\"], normalize_embeddings=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dim_reduction(embeddings):\n",
    "    cov = np.cov(embeddings.T)\n",
    "    eig_vals, eig_vecs = np.linalg.eig(cov)\n",
    "\n",
    "    eig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:,i]) for i in range(len(eig_vals))]\n",
    "    eig_pairs.sort(key=lambda x: x[0], reverse=True)\n",
    "    tot = sum(eig_vals)\n",
    "    var_exp = [(i / tot)*100 for i in sorted(eig_vals, reverse=True)]\n",
    "    cum_var_exp = np.cumsum(var_exp)\n",
    "    var90_ind = np.argmax((cum_var_exp > 90))\n",
    "    proj_matrix = np.hstack([eig_pairs[i][1].reshape(-1, 1) for i in range(var90_ind + 1)])\n",
    "    \n",
    "    return proj_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj_matrix = dim_reduction(embeddings)\n",
    "embeddings = np.dot(embeddings, proj_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56803, 209)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "qdrant_client = QdrantClient(\"http://localhost:6333\")\n",
    "collection_name = \"video_segments\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qdrant_client.recreate_collection(\n",
    "    collection_name=collection_name,\n",
    "    vectors_config=VectorParams(\n",
    "        size=embeddings.shape[1],\n",
    "        distance=Distance.COSINE,\n",
    "        quantization_config=ScalarQuantization(\n",
    "            scalar=ScalarQuantizationConfig(\n",
    "                type=ScalarType.INT8,\n",
    "                always_ram=True\n",
    "            )\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing points: 100%|██████████| 56803/56803 [01:38<00:00, 574.38it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully loaded into Qdrant.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1000 \n",
    "\n",
    "# Extract video_id and timestamp for the video segments\n",
    "filenames, starts, ends = [], [], []\n",
    "for i in range(tedlium[\"train\"].num_rows):\n",
    "    filename, start, end, _ = tedlium['train'][i]['id'].split('-')\n",
    "    filenames.append(filename)\n",
    "    starts.append(float(start))\n",
    "    ends.append(float(end))\n",
    "\n",
    "points = []\n",
    "for i, record in tqdm(enumerate(tedlium[\"train\"]), total=len(tedlium[\"train\"]), desc=\"Processing points\"):\n",
    "    embedding = embeddings[i]  \n",
    "    text = record[\"text\"]\n",
    "    \n",
    "    point = PointStruct(\n",
    "        id=i,\n",
    "        vector=embedding,\n",
    "        payload={\n",
    "            \"video_id\": filenames[i],\n",
    "            \"timestamp\": f\"{starts[i]:.2f}-{ends[i]:.2f}\",\n",
    "            \"text\": text\n",
    "        }\n",
    "    )\n",
    "    points.append(point)\n",
    "    \n",
    "    if len(points) >= batch_size:\n",
    "        qdrant_client.upsert(collection_name=collection_name, points=points)\n",
    "        points = []\n",
    "\n",
    "# Insert remaining points\n",
    "if points:\n",
    "    qdrant_client.upsert(collection_name=collection_name, points=points)\n",
    "\n",
    "print(\"Data successfully loaded into Qdrant.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_audio(video_path):\n",
    "    video = mp.VideoFileClip(video_path)\n",
    "    audio = video.audio\n",
    "    audio_path = video_path.rsplit(\".\", 1)[0] + \".wav\"\n",
    "    audio.write_audiofile(audio_path)\n",
    "    return audio_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "whisper_model = WhisperModel(\"base.en\", device=\"cpu\", compute_type=\"int8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_audio(audio_path, whisper_model):\n",
    "    segments, info = whisper_model.transcribe(audio_path, word_timestamps=True)\n",
    "    return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_upload_video(file_path, whisper_model):\n",
    "    video_extensions = ('.mp4', '.avi', '.mov', '.mkv')\n",
    "    audio_extensions = ('.wav', '.mp3', '.flac')\n",
    "\n",
    "    if file_path.lower().endswith(video_extensions):\n",
    "        audio_path = extract_audio(file_path)\n",
    "    elif file_path.lower().endswith(audio_extensions):\n",
    "        audio_path = file_path\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file format. Please provide a video or audio file.\")\n",
    "    \n",
    "    print(\"Loading audio file...\")\n",
    "    # Use the correct method based on the file extension\n",
    "    if audio_path.lower().endswith('.wav'):\n",
    "        audio = AudioSegment.from_wav(audio_path)\n",
    "    elif audio_path.lower().endswith('.mp3'):\n",
    "        audio = AudioSegment.from_mp3(audio_path)\n",
    "    elif audio_path.lower().endswith('.flac'):\n",
    "        audio = AudioSegment.from_file(audio_path, format=\"flac\")\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported audio format. Please provide a WAV, MP3, or FLAC file.\")\n",
    "\n",
    "    chunk_length_ms = 10 * 1000  # 10 seconds in milliseconds\n",
    "    chunks = make_chunks(audio, chunk_length_ms)\n",
    "\n",
    "    points = []\n",
    "    base_id = uuid.uuid4().int >> 64\n",
    "    processed_chunks = []\n",
    "    \n",
    "    print(f\"Processing {len(chunks)} chunks...\")\n",
    "    for i, chunk in tqdm(enumerate(chunks), total=len(chunks), desc=\"Processing chunks\"):\n",
    "        chunk_path = f\"temp_chunk_{i}.wav\"\n",
    "        chunk.export(chunk_path, format=\"wav\")\n",
    "\n",
    "        segments = transcribe_audio(chunk_path, whisper_model)\n",
    "        if segments:\n",
    "            text = \" \".join([segment.text for segment in segments])\n",
    "            start_time = i * 10  # Start time in seconds\n",
    "            end_time = (i + 1) * 10  # End time in seconds\n",
    "            \n",
    "            processed_chunks.append({\n",
    "                \"id\": base_id + i,\n",
    "                \"video_id\": os.path.basename(file_path),\n",
    "                \"timestamp\": f\"{start_time:.2f}-{end_time:.2f}\",\n",
    "                \"text\": text,\n",
    "                \"audio_chunk\": chunk\n",
    "            })\n",
    "\n",
    "            # Generate embedding\n",
    "            embedding = model.encode(text, normalize_embeddings=True)\n",
    "            embedding = np.dot(embedding, proj_matrix)\n",
    "\n",
    "            point = PointStruct(\n",
    "                id=base_id + i,\n",
    "                vector=embedding.tolist(),\n",
    "                payload={\n",
    "                    \"video_id\": os.path.basename(file_path),\n",
    "                    \"timestamp\": f\"{start_time:.2f}-{end_time:.2f}\",\n",
    "                    \"text\": text\n",
    "                }\n",
    "            )\n",
    "            points.append(point)\n",
    "\n",
    "        os.remove(chunk_path)\n",
    "\n",
    "        if len(points) >= 1000:\n",
    "            qdrant_client.upsert(collection_name=collection_name, points=points)\n",
    "            points = []\n",
    "\n",
    "    if points:\n",
    "        qdrant_client.upsert(collection_name=collection_name, points=points)\n",
    "\n",
    "    print(f\"Video {file_path} processed and uploaded to Qdrant.\")\n",
    "\n",
    "    if file_path.lower().endswith(video_extensions):\n",
    "        os.remove(audio_path)\n",
    "        \n",
    "    return processed_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_and_display_chunk(chunk):\n",
    "    print(f\"Chunk ID: {chunk['id']}\")\n",
    "    print(f\"Video ID: {chunk['video_id']}\")\n",
    "    print(f\"Timestamp: {chunk['timestamp']}\")\n",
    "    print(f\"Text: {chunk['text']}\")\n",
    "    \n",
    "    # Convert the audio chunk to a numpy array\n",
    "    audio_data = np.array(chunk['audio_chunk'].get_array_of_samples())\n",
    "    \n",
    "    # Play the audio\n",
    "    play_obj = sa.play_buffer(\n",
    "        audio_data.tobytes(),\n",
    "        num_channels=chunk['audio_chunk'].channels,\n",
    "        bytes_per_sample=chunk['audio_chunk'].sample_width,\n",
    "        sample_rate=chunk['audio_chunk'].frame_rate\n",
    "    )\n",
    "    \n",
    "    # Wait for playback to finish\n",
    "    play_obj.wait_done()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_qdrant_entry(chunk_id):\n",
    "    # Retrieve the point from Qdrant\n",
    "    points = qdrant_client.retrieve(\n",
    "        collection_name=collection_name,\n",
    "        ids=[chunk_id]\n",
    "    )\n",
    "    \n",
    "    if points:\n",
    "        point = points[0]\n",
    "        print(\"Data in Qdrant:\")\n",
    "        print(f\"ID: {point.id}\")\n",
    "        print(f\"Video ID: {point.payload['video_id']}\")\n",
    "        print(f\"Timestamp: {point.payload['timestamp']}\")\n",
    "        print(f\"Text: {point.payload['text']}\")\n",
    "    else:\n",
    "        print(f\"No data found for chunk ID {chunk_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in multimedia/tom_hardy_interview.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Loading audio file...\n",
      "Processing 19 chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 19/19 [00:35<00:00,  1.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video multimedia/tom_hardy_interview.mp4 processed and uploaded to Qdrant.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "video_path = \"multimedia/tom_hardy_interview.mp4\"\n",
    "processed_chunks = process_and_upload_video(video_path, whisper_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk ID: 3993838373608836783\n",
      "Video ID: tom_hardy_interview.mp4\n",
      "Timestamp: 0.00-10.00\n",
      "Text:  You\n",
      "Data in Qdrant:\n",
      "ID: 3993838373608836783\n",
      "Video ID: tom_hardy_interview.mp4\n",
      "Timestamp: 0.00-10.00\n",
      "Text:  You\n",
      "Chunk ID: 3993838373608836784\n",
      "Video ID: tom_hardy_interview.mp4\n",
      "Timestamp: 10.00-20.00\n",
      "Text:  My relationship with Charlie grew from phone calls only first.\n",
      "Data in Qdrant:\n",
      "ID: 3993838373608836784\n",
      "Video ID: tom_hardy_interview.mp4\n",
      "Timestamp: 10.00-20.00\n",
      "Text:  My relationship with Charlie grew from phone calls only first.\n"
     ]
    }
   ],
   "source": [
    "# Play and display each chunk, and verify its entry in Qdrant\n",
    "for chunk in processed_chunks:\n",
    "    play_and_display_chunk(chunk)\n",
    "    verify_qdrant_entry(chunk['id'])\n",
    "    \n",
    "    # Ask user if they want to continue to the next chunk\n",
    "    if input(\"Press Enter to continue to the next chunk, or type 'q' to quit: \").lower() == 'q':\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert from seconds to MM:SS format\n",
    "def seconds_to_hms(seconds):\n",
    "    minutes, seconds = divmod(seconds, 60)\n",
    "    if minutes < 60:\n",
    "        return f\"{int(minutes):02d}:{int(seconds):02d}\"\n",
    "    else:\n",
    "        hours, minutes = divmod(minutes, 60)\n",
    "        return f\"{int(hours):02d}:{int(minutes):02d}:{int(seconds):02d}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_model = SentenceTransformer(\"Alibaba-NLP/gte-large-en-v1.5\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discounted_cumulative_gain_k(relevance, k=3):\n",
    "    return np.sum(relevance / np.log2(np.arange(2, k + 2)))\n",
    "\n",
    "def normalized_discounted_cumulative_gain_k(relevance, k=3):\n",
    "    dcg_k = discounted_cumulative_gain_k(relevance, k)\n",
    "    idcg_k = discounted_cumulative_gain_k(np.ones(k), k)\n",
    "    return dcg_k / idcg_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_precision_k(relevance, k=3):\n",
    "        r = relevance[:k].sum()\n",
    "        if r == 0:\n",
    "            return 0\n",
    "        numerator = np.cumsum(relevance[:k]) * relevance[:k]\n",
    "        denominator = r * np.arange(1, k + 1)\n",
    "        ap_k = (numerator / denominator).sum()\n",
    "        return ap_k\n",
    "\n",
    "def compute_metrics(gt, threshold=0.7, list_k=[1, 3, 5]):\n",
    "    metrics = dict()\n",
    "    relevance = (gt > threshold).flatten()\n",
    "    for k in list_k:\n",
    "        ap = average_precision_k(relevance, k)\n",
    "        recall = sum(relevance[:k]) / (sum(relevance) + 1e-9)\n",
    "        hit_rate = sum(relevance[:k]) > 0\n",
    "        ndcg = normalized_discounted_cumulative_gain_k(relevance[:k], k)\n",
    "    \n",
    "        metrics[k] = {'AP': ap, 'recall': recall, 'hit_rate': hit_rate, 'ndcg': ndcg}\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_similar_segments(query_text, top_k=10, ef=128, list_k=[1, 3, 5], proj_matrix=None):\n",
    "    start_time = time.time()\n",
    "    query_embedding = model.encode(query_text).tolist()\n",
    "    if proj_matrix is not None:\n",
    "        query_embedding = np.dot(query_embedding, proj_matrix)\n",
    "    \n",
    "    search_result = qdrant_client.search(\n",
    "        collection_name=collection_name,\n",
    "        query_vector=query_embedding,\n",
    "        limit=top_k,\n",
    "        search_params=SearchParams(\n",
    "            hnsw_ef=ef  \n",
    "        )\n",
    "    )\n",
    "    \n",
    "    end_time = time.time()\n",
    "    inference_time = end_time - start_time\n",
    "    \n",
    "    similarity = meta_model.similarity(meta_model.encode(query_text, normalize_embeddings=True), meta_model.encode([result.payload[\"text\"] for result in search_result], normalize_embeddings=True))\n",
    "    \n",
    "    return search_result, inference_time, compute_metrics(similarity.numpy(), list_k=list_k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search completed in 0.6002 seconds\n",
      "Score: 0.8502\n",
      "Text:  And sometimes you've got to cut a little piece of yourself off, no matter how much it\n",
      "Video ID: tom_hardy_interview.mp4\n",
      "Timestamp: 150.00-160.00\n",
      "Converted timestamp: 02:30 - 02:40\n",
      "\n",
      "Score: 0.5951\n",
      "Text:   but  this is how i  do work  i do  take pieces and bits and  look at  it  and struggle with it  and  cut it away and of  course it's  not going to look like  that  but  it is  the  crazy way i  tend to\n",
      "Video ID: FrankGehry_1990\n",
      "Timestamp: 2512.61-2525.42\n",
      "Converted timestamp: 41:52 - 42:05\n",
      "\n",
      "Score: 0.5438\n",
      "Text: to laser  cut my \n",
      "Video ID: MarianBantjes_2010\n",
      "Timestamp: 729.35-730.91\n",
      "Converted timestamp: 12:09 - 12:10\n",
      "\n",
      "Score: 0.5360\n",
      "Text: you lay down a couple of very simple rules  always cut away from your body  keep the blade sharp  never force it  and  these are things kids can understand and practice with and  yeah they're going to cut themselves  i have some terrible scars on my legs from where i stabbed myself\n",
      "Video ID: GeverTulley_2007U\n",
      "Timestamp: 293.96-308.39\n",
      "Converted timestamp: 04:53 - 05:08\n",
      "\n",
      "Score: 0.5352\n",
      "Text: is that  you  don't have to make yourself \n",
      "Video ID: ChrisJordan_2008\n",
      "Timestamp: 578.05-579.61\n",
      "Converted timestamp: 09:38 - 09:39\n",
      "\n",
      "Score: 0.4967\n",
      "Text: you know  i am so bad  at  tech  that  my  daughter who is now forty one  when she was five  was overheard by me  to say to a friend of hers  if it doesn't bleed when you  cut it  my daddy doesn't understand \n",
      "Video ID: SherwinNuland_2003\n",
      "Timestamp: 13.69-28.32\n",
      "Converted timestamp: 00:13 - 00:28\n",
      "\n",
      "Score: 0.4909\n",
      "Text: to  inwardly  like in your head  that is  to tidy that up \n",
      "Video ID: UrsusWehrli_2006\n",
      "Timestamp: 684.14-687.58\n",
      "Converted timestamp: 11:24 - 11:27\n",
      "\n",
      "Score: 0.4779\n",
      "Text: and i  cut  it bent it  round  and  \n",
      "Video ID: WillardWigan_2009G\n",
      "Timestamp: 889.30-891.33\n",
      "Converted timestamp: 14:49 - 14:51\n",
      "\n",
      "Score: 0.4737\n",
      "Text: really  important in this stuff is  as  we  cut  we also have to grow \n",
      "Video ID: JuanEnriquez_2009\n",
      "Timestamp: 428.88-432.48\n",
      "Converted timestamp: 07:08 - 07:12\n",
      "\n",
      "Score: 0.4705\n",
      "Text:  which i do sometimes \n",
      "Video ID: BillyGraham_1998\n",
      "Timestamp: 907.64-909.16\n",
      "Converted timestamp: 15:07 - 15:09\n",
      "\n",
      "{1: {'AP': 1.0, 'recall': 0.9999999989999999, 'hit_rate': True, 'ndcg': 1.0}, 3: {'AP': 1.0, 'recall': 0.9999999989999999, 'hit_rate': True, 'ndcg': 0.46927872602275644}, 5: {'AP': 1.0, 'recall': 0.9999999989999999, 'hit_rate': True, 'ndcg': 0.3391602052736161}}\n"
     ]
    }
   ],
   "source": [
    "# Example query\n",
    "query = \"sometimes you gotta cut a little piece of yourself\"\n",
    "results, search_time, metrics = search_similar_segments(query, proj_matrix=proj_matrix)\n",
    "\n",
    "print(f\"Search completed in {search_time:.4f} seconds\")\n",
    "for result in results:\n",
    "    print(f\"Score: {result.score:.4f}\")\n",
    "    print(f\"Text: {result.payload['text']}\")\n",
    "    print(f\"Video ID: {result.payload['video_id']}\")\n",
    "    print(f\"Timestamp: {result.payload['timestamp']}\")\n",
    "    print(f\"Converted timestamp: {seconds_to_hms(float(result.payload['timestamp'].split('-')[0]))} - {seconds_to_hms(float(result.payload['timestamp'].split('-')[1]))}\")\n",
    "    print()\n",
    "    \n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example query\n",
    "list_k = [1, 3, 5]\n",
    "metrics_list = dict([[k, dict([[m, []] for m in ['AP', 'recall', 'hit_rate', 'ndcg']])] for k in list_k])\n",
    "times_list = []\n",
    "\n",
    "for sample in tqdm(queries, total=len(queries), desc=\"Calculating metrics\"):\n",
    "    query = sample['query']\n",
    "    results, search_time, sample_metrics = search_similar_segments(query, proj_matrix=proj_matrix)\n",
    "    times_list.append(search_time)\n",
    "    for k in sample_metrics:\n",
    "        for metric in sample_metrics[k]:\n",
    "            metrics_list[k][metric].append(sample_metrics[k][metric])\n",
    "            \n",
    "metrics = dict([[k, dict([[m, 0] for m in ['AP', 'recall', 'hit_rate', 'ndcg']])] for k in list_k])\n",
    "times = sum(times_list) / len(times_list)\n",
    "for k in metrics_list:\n",
    "    for metric in metrics_list[k]:\n",
    "        metrics[k][metric] = sum(metrics_list[k][metric]) / len(metrics_list[k][metric])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "a_level_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
